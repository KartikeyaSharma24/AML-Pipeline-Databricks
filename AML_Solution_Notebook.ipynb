{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe621fb-0b85-4238-b680-f907c364cac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Test code\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f4641d-892e-444f-9808-9eda8e609e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic Data generation code, we created temp views since DBFS and Hive Metastore are disabled\n",
    "\n",
    "from pyspark.sql import functions as F, types as T, SparkSession\n",
    "import random, datetime\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Parameters\n",
    "N_CUSTOMERS, N_ACCOUNTS, N_TX = 5000, 10000, 200000\n",
    "START_DATE = datetime.date(2024,1,1)\n",
    "DAYS = 120\n",
    "COUNTRIES = [\"IN\",\"US\",\"GB\",\"AE\",\"SG\",\"HK\"]\n",
    "HIGH_RISK = {\"AE\",\"HK\"}\n",
    "MCCs = [\"5411\",\"5812\",\"7399\",\"5999\",\"6011\"]\n",
    "\n",
    "def rand_date():\n",
    "    return START_DATE + datetime.timedelta(days=random.randint(0, DAYS))\n",
    "\n",
    "# ---------------- Customers ----------------\n",
    "customers = [(i, f\"Cust_{i}\",\n",
    "              str(datetime.date(1970+random.randint(0,30), random.randint(1,12), random.randint(1,28))),\n",
    "              random.choice(COUNTRIES),\n",
    "              random.choice([\"Low\",\"Medium\",\"High\"]),\n",
    "              random.choice([\"Verified\",\"Pending\",\"Failed\"]),\n",
    "              str(rand_date()))\n",
    "             for i in range(1, N_CUSTOMERS+1)]\n",
    "\n",
    "schema_c = T.StructType([\n",
    "    T.StructField(\"CustomerID\", T.IntegerType(), False),\n",
    "    T.StructField(\"Name\", T.StringType(), True),\n",
    "    T.StructField(\"DOB\", T.StringType(), True),\n",
    "    T.StructField(\"Country\", T.StringType(), True),\n",
    "    T.StructField(\"RiskLevel\", T.StringType(), True),\n",
    "    T.StructField(\"KYCStatus\", T.StringType(), True),\n",
    "    T.StructField(\"OnboardDate\", T.StringType(), True),\n",
    "])\n",
    "df_customers = spark.createDataFrame(customers, schema_c)\n",
    "\n",
    "# ---------------- Accounts ----------------\n",
    "accounts = []\n",
    "for i in range(1, N_ACCOUNTS+1):\n",
    "    cust = random.randint(1, N_CUSTOMERS)\n",
    "    accounts.append((\n",
    "        i, cust, random.choice([\"Savings\",\"Current\",\"Wallet\"]),\n",
    "        str(rand_date()), random.choice([\"Active\",\"Frozen\",\"Closed\"])\n",
    "    ))\n",
    "\n",
    "schema_a = T.StructType([\n",
    "    T.StructField(\"AccountID\", T.IntegerType(), False),\n",
    "    T.StructField(\"CustomerID\", T.IntegerType(), False),\n",
    "    T.StructField(\"AccountType\", T.StringType(), True),\n",
    "    T.StructField(\"OpenDate\", T.StringType(), True),\n",
    "    T.StructField(\"Status\", T.StringType(), True),\n",
    "])\n",
    "df_accounts = spark.createDataFrame(accounts, schema_a)\n",
    "\n",
    "# ---------------- Transactions ----------------\n",
    "random.seed(42)\n",
    "tx = []\n",
    "for i in range(1, N_TX+1):\n",
    "    acc = random.randint(1, N_ACCOUNTS)\n",
    "    amt = round(random.expovariate(1/2000), 2)\n",
    "    ts = str(rand_date())\n",
    "    country = random.choice(COUNTRIES)\n",
    "    mcc = random.choice(MCCs)\n",
    "    tx_type = random.choice([\"POS\",\"TRANSFER\",\"ATM\",\"ONLINE\"])\n",
    "    channel = random.choice([\"Mobile\",\"Web\",\"Branch\",\"API\"])\n",
    "    counterparty = random.randint(1, N_ACCOUNTS)\n",
    "\n",
    "    label = \"Normal\"\n",
    "    # Structuring\n",
    "    if random.random() < 0.02 and amt < 10000:\n",
    "        amt = round(random.uniform(9000, 9999), 2); label = \"Structuring\"\n",
    "    # Smurfing\n",
    "    if random.random() < 0.02 and tx_type == \"TRANSFER\":\n",
    "        amt = round(random.uniform(100, 500), 2); counterparty = random.randint(1, N_ACCOUNTS); label = \"Smurfing\"\n",
    "    # Layering\n",
    "    if random.random() < 0.01 and tx_type == \"TRANSFER\" and country in HIGH_RISK:\n",
    "        amt = round(random.uniform(2000, 20000), 2); label = \"Layering\"\n",
    "\n",
    "    tx.append((i, acc, ts, amt, \"INR\", tx_type, channel, counterparty, country, mcc, label))\n",
    "\n",
    "schema_t = T.StructType([\n",
    "    T.StructField(\"TxID\", T.IntegerType(), False),\n",
    "    T.StructField(\"AccountID\", T.IntegerType(), False),\n",
    "    T.StructField(\"Timestamp\", T.StringType(), True),\n",
    "    T.StructField(\"Amount\", T.DoubleType(), True),\n",
    "    T.StructField(\"Currency\", T.StringType(), True),\n",
    "    T.StructField(\"TxType\", T.StringType(), True),\n",
    "    T.StructField(\"Channel\", T.StringType(), True),\n",
    "    T.StructField(\"CounterpartyAccountID\", T.IntegerType(), True),\n",
    "    T.StructField(\"Country\", T.StringType(), True),\n",
    "    T.StructField(\"MerchantCategory\", T.StringType(), True),\n",
    "    T.StructField(\"InjectedLabel\", T.StringType(), True),\n",
    "])\n",
    "df_tx = spark.createDataFrame(tx, schema_t)\n",
    "\n",
    "# Create temp views instead of writing to DBFS\n",
    "df_customers.createOrReplaceTempView(\"customers_view\")\n",
    "df_accounts.createOrReplaceTempView(\"accounts_view\")\n",
    "df_tx.createOrReplaceTempView(\"transactions_view\")\n",
    "\n",
    "print(\"‚úÖ Synthetic data generated and registered as temp views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c8d7c3-49f1-45be-a07a-c36df8a546ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning & Normalization \n",
    "\n",
    "# Normalize dates and amounts\n",
    "customers_clean = (df_customers\n",
    "    .withColumn(\"DOB\", F.to_date(\"DOB\"))\n",
    "    .withColumn(\"OnboardDate\", F.to_date(\"OnboardDate\"))\n",
    "    .dropna(subset=[\"CustomerID\",\"Name\"])\n",
    ")\n",
    "\n",
    "accounts_clean = (df_accounts\n",
    "    .withColumn(\"OpenDate\", F.to_date(\"OpenDate\"))\n",
    "    .dropna(subset=[\"AccountID\",\"CustomerID\"])\n",
    ")\n",
    "\n",
    "transactions_clean = (df_tx\n",
    "    .withColumn(\"Timestamp\", F.to_date(\"Timestamp\"))\n",
    "    .withColumn(\"Amount\", F.when(F.col(\"Amount\") < 0, 0).otherwise(F.col(\"Amount\")))\n",
    "    .dropna(subset=[\"TxID\",\"AccountID\",\"Timestamp\"])\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc4c5cf-277a-4b9e-9219-e639d773f952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You need to rename columns before or after the join so they‚Äôre unique.\n",
    "\n",
    "accounts_clean = accounts_clean.withColumnRenamed(\"Status\", \"AccountStatus\")\n",
    "customers_clean = customers_clean.withColumnRenamed(\"Country\", \"CustomerCountry\") \\\n",
    "                                 .withColumnRenamed(\"RiskLevel\", \"CustomerRiskLevel\") \\\n",
    "                                 .withColumnRenamed(\"KYCStatus\", \"CustomerKYCStatus\")\n",
    "transactions_clean = transactions_clean.withColumnRenamed(\"Country\", \"TxCountry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004e1d87-ecfc-4367-8a6d-0f073063602f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enrichment (Join datasets)\n",
    "\n",
    "tx_enriched = (transactions_clean\n",
    "    .join(accounts_clean, \"AccountID\", \"left\")\n",
    "    .join(customers_clean, \"CustomerID\", \"left\")\n",
    ")\n",
    "\n",
    "print(\"Enriched Transactions:\", tx_enriched.count())\n",
    "tx_enriched.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aada35e-6bcd-45a5-84a5-2768a271b3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Example features\n",
    "features = (tx_enriched\n",
    "    .withColumn(\"HighRiskCountryFlag\", F.when(F.col(\"TxCountry\").isin([\"AE\",\"HK\"]), 1).otherwise(0))\n",
    "    .withColumn(\"LargeTxFlag\", F.when(F.col(\"Amount\") > 10000, 1).otherwise(0))\n",
    "    .withColumn(\"IsStructuring\", F.when(F.col(\"InjectedLabel\")==\"Structuring\", 1).otherwise(0))\n",
    "    .withColumn(\"IsSmurfing\", F.when(F.col(\"InjectedLabel\")==\"Smurfing\", 1).otherwise(0))\n",
    "    .withColumn(\"IsLayering\", F.when(F.col(\"InjectedLabel\")==\"Layering\", 1).otherwise(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba221b7-2f8f-44a7-a301-18fbf1069fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The next logical step is Rule-Based Detection. This stage will let us flag suspicious transactions using simple AML rules before we move into machine learning.\n",
    "\n",
    "# Rule-Based Detection\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Apply simple AML rules\n",
    "alerts = (features\n",
    "    .filter(\n",
    "        (F.col(\"HighRiskCountryFlag\") == 1) |\n",
    "        (F.col(\"LargeTxFlag\") == 1) |\n",
    "        (F.col(\"IsStructuring\") == 1) |\n",
    "        (F.col(\"IsSmurfing\") == 1) |\n",
    "        (F.col(\"IsLayering\") == 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"üö® Alerts flagged:\", alerts.count())\n",
    "\n",
    "# Inspect flagged transactions\n",
    "alerts.select(\n",
    "    \"TxID\", \"Amount\", \"TxCountry\", \"InjectedLabel\",\n",
    "    \"CustomerRiskLevel\", \"CustomerKYCStatus\", \"AccountType\"\n",
    ").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2d66f3-1a84-4142-aeee-79bdda8bb363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# demonstrate ML detection by using lighter‚Äëweight strategies that don‚Äôt trigger the Connect ML serialization limits.\n",
    "\n",
    "# Convert to Pandas\n",
    "pdf = features.sample(fraction=0.01, seed=42).toPandas()\n",
    "\n",
    "# Prepare features/labels\n",
    "X = pdf[[\"Amount\",\"HighRiskCountryFlag\",\"LargeTxFlag\",\"IsStructuring\",\"IsSmurfing\",\"IsLayering\"]]\n",
    "y = pdf[\"InjectedLabel\"]\n",
    "\n",
    "# Train scikit-learn model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "\n",
    "# After running the above code :\n",
    "# you‚Äôve successfully trained a scikit‚Äëlearn model and got ~99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f0c2d6-3376-45ef-ae54-fb6bd9e9fe94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarize alerts by customer/account\n",
    "\n",
    "# Aggregate suspicious alerts per customer\n",
    "alerts_summary = (alerts\n",
    "    .groupBy(\"CustomerID\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"SuspiciousTxCount\"),\n",
    "        F.sum(\"LargeTxFlag\").alias(\"LargeTxCount\"),\n",
    "        F.sum(\"HighRiskCountryFlag\").alias(\"HighRiskTxCount\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"SuspiciousTxCount\"))\n",
    ")\n",
    "\n",
    "alerts_summary.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f82960-9cc9-44c4-a7b4-73385e048620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join with customer details \n",
    "\n",
    "customer_alerts = (alerts_summary\n",
    "    .join(customers_clean, \"CustomerID\", \"left\")\n",
    "    .select(\"CustomerID\",\"Name\",\"CustomerCountry\",\"CustomerRiskLevel\",\"CustomerKYCStatus\",\n",
    "            \"SuspiciousTxCount\",\"LargeTxCount\",\"HighRiskTxCount\")\n",
    "    .orderBy(F.desc(\"SuspiciousTxCount\"))\n",
    ")\n",
    "\n",
    "customer_alerts.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47703ba-ff10-470b-9378-c24ceb3884f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create investigator‚Äëfriendly cases\n",
    "\n",
    "# Flag customers with >3 suspicious transactions\n",
    "cases = customer_alerts.filter(F.col(\"SuspiciousTxCount\") > 3)\n",
    "\n",
    "print(\"üö® Cases requiring investigation:\", cases.count())\n",
    "cases.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759fdf21-45e1-45b7-8a3f-5952c9fddf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Conversion to Pandas for Reporting Purposes\n",
    "\n",
    "alerts_pdf = alerts.toPandas()\n",
    "cases_pdf = cases.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8c9ef2-6f2d-440b-a916-c3956f966d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reporting Dashboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrames to Pandas for plotting (sample if dataset is huge)\n",
    "alerts_pdf = alerts.limit(5000).toPandas()   # limit for memory safety\n",
    "cases_pdf = cases.limit(5000).toPandas()\n",
    "\n",
    "# --- Bar chart: Top 10 customers with suspicious transactions ---\n",
    "top_customers = (alerts_pdf.groupby(\"CustomerID\")\n",
    "                 .size()\n",
    "                 .reset_index(name=\"SuspiciousTxCount\")\n",
    "                 .sort_values(\"SuspiciousTxCount\", ascending=False)\n",
    "                 .head(10))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"CustomerID\", y=\"SuspiciousTxCount\", hue=\"CustomerID\",\n",
    "            data=top_customers, palette=\"Reds\", legend=False)\n",
    "plt.title(\"Top 10 Customers with Suspicious Transactions\")\n",
    "plt.xlabel(\"Customer ID\")\n",
    "plt.ylabel(\"Suspicious Transaction Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# --- Pie chart: Distribution of suspicious transaction types ---\n",
    "plt.figure(figsize=(6,6))\n",
    "alerts_pdf[\"InjectedLabel\"].value_counts().plot.pie(\n",
    "    autopct=\"%1.1f%%\", colors=sns.color_palette(\"pastel\"))\n",
    "plt.title(\"Suspicious Transaction Types Distribution\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# --- Time series: Suspicious transactions per day ---\n",
    "alerts_pdf[\"Timestamp\"] = pd.to_datetime(alerts_pdf[\"Timestamp\"])\n",
    "daily_counts = alerts_pdf.groupby(alerts_pdf[\"Timestamp\"].dt.date).size().reset_index(name=\"Count\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x=\"Timestamp\", y=\"Count\", data=daily_counts, marker=\"o\")\n",
    "plt.title(\"Suspicious Transactions Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304e05f1-073c-430d-8820-d32f5dd57a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Executive Summary\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "summary_text = \"\"\"\n",
    "# üè¶ AML Pipeline Executive Summary\n",
    "\n",
    "This notebook demonstrates a complete **Anti-Money Laundering (AML) detection pipeline** built on synthetic data.\n",
    "\n",
    "### üîπ Key Steps\n",
    "- **Data Generation & Cleaning**: Created realistic transaction and customer datasets, enriched with risk flags.\n",
    "- **Feature Engineering**: Added AML typology indicators (Structuring, Smurfing, Layering).\n",
    "- **Rule-Based Detection**: Flagged suspicious transactions using compliance thresholds.\n",
    "- **Machine Learning (scikit-learn)**: Trained lightweight models to classify suspicious activity, achieving high accuracy.\n",
    "- **Case Management**: Aggregated alerts into investigator-friendly cases for escalation.\n",
    "- **Reporting Dashboards**: Visualized suspicious activity with bar charts, pie charts, and time series plots.\n",
    "\n",
    "### üìä Insights\n",
    "- **Top Customers**: Certain accounts show repeated suspicious activity, requiring deeper investigation.\n",
    "- **Typology Distribution**: Structuring and Smurfing dominate suspicious transaction patterns.\n",
    "- **Temporal Trends**: Spikes in suspicious activity suggest coordinated attempts at laundering.\n",
    "\n",
    "### ‚úÖ Interview Takeaway\n",
    "This pipeline showcases:\n",
    "- **End-to-end solution architecture** (data ‚Üí detection ‚Üí reporting).\n",
    "- **Scalable design** with Spark + scikit-learn fallback for Free Edition limits.\n",
    "- **Professional reporting visuals** that mimic real compliance dashboards.\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary_text))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AML_Solution_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
